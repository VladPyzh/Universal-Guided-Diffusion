{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict, Optional, Union, Tuple\n",
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    xmin: int\n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "\n",
    "    @property\n",
    "    def xyxy(self) -> List[float]:\n",
    "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    score: float\n",
    "    label: str\n",
    "    box: BoundingBox\n",
    "    mask: Optional[np.array] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
    "        return cls(score=detection_dict['score'],\n",
    "                   label=detection_dict['label'],\n",
    "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
    "                                   ymin=detection_dict['box']['ymin'],\n",
    "                                   xmax=detection_dict['box']['xmax'],\n",
    "                                   ymax=detection_dict['box']['ymax']))\n",
    "\n",
    "def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Extract the vertices of the contour\n",
    "    polygon = largest_contour.reshape(-1, 2).tolist()\n",
    "\n",
    "    return polygon\n",
    "\n",
    "def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a polygon to a segmentation mask.\n",
    "\n",
    "    Args:\n",
    "    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n",
    "    - image_shape (tuple): Shape of the image (height, width) for the mask.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Segmentation mask with the polygon filled.\n",
    "    \"\"\"\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Convert polygon to an array of points\n",
    "    pts = np.array(polygon, dtype=np.int32)\n",
    "\n",
    "    # Fill the polygon with white color (255)\n",
    "    cv2.fillPoly(mask, [pts], color=(255,))\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_boxes(results: DetectionResult) -> List[List[List[float]]]:\n",
    "    boxes = []\n",
    "    for result in results:\n",
    "        xyxy = result.box.xyxy\n",
    "        boxes.append(xyxy)\n",
    "\n",
    "    return [boxes]\n",
    "\n",
    "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
    "    masks = masks.cpu().float()\n",
    "    masks = masks.permute(0, 2, 3, 1)\n",
    "    masks = masks.mean(axis=-1)\n",
    "    masks = (masks > 0).int()\n",
    "    masks = masks.numpy().astype(np.uint8)\n",
    "    masks = list(masks)\n",
    "\n",
    "    if polygon_refinement:\n",
    "        for idx, mask in enumerate(masks):\n",
    "            shape = mask.shape\n",
    "            polygon = mask_to_polygon(mask)\n",
    "            mask = polygon_to_mask(polygon, shape)\n",
    "            masks[idx] = mask\n",
    "\n",
    "    return masks\n",
    "\n",
    "class SegmentationEngine:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.detector_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "        self.object_detector = pipeline(model=self.detector_id, \n",
    "                                        task=\"zero-shot-object-detection\", \n",
    "                                        device=self.device)\n",
    "\n",
    "        self.segmenter_id = \"facebook/sam-vit-huge\"\n",
    "\n",
    "        self.segmentator = AutoModelForMaskGeneration.from_pretrained(self.segmenter_id).to(self.device)\n",
    "        self.seg_processor = AutoProcessor.from_pretrained(self.segmenter_id)\n",
    "\n",
    "\n",
    "    def detect(\n",
    "            self,\n",
    "            image: Image.Image,\n",
    "            labels: List[str],\n",
    "            threshold: float = 0.5,\n",
    "    ):\n",
    "        \n",
    "        labels = [label if label.endswith(\".\") else label+\".\" for label in labels]\n",
    "\n",
    "        results = self.object_detector(image,  candidate_labels=labels, threshold=threshold)\n",
    "        results = [DetectionResult.from_dict(result) for result in results]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def segment(\n",
    "            self,\n",
    "            image: Image.Image,\n",
    "            detection_results: List[Dict[str, Any]],\n",
    "            polygon_refinement: bool = False,\n",
    "            segmenter_id: Optional[str] = None,\n",
    "    ):\n",
    "        boxes = get_boxes(detection_results)\n",
    "        inputs = self.seg_processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        outputs = self.segmentator(**inputs)\n",
    "        masks = self.seg_processor.post_process_masks(\n",
    "            masks=outputs.pred_masks,\n",
    "            original_sizes=inputs.original_sizes,\n",
    "            reshaped_input_sizes=inputs.reshaped_input_sizes\n",
    "        )[0]\n",
    "\n",
    "        masks = refine_masks(masks, polygon_refinement)\n",
    "\n",
    "        for detection_result, mask in zip(detection_results, masks):\n",
    "            detection_result.mask = mask\n",
    "\n",
    "        return detection_results\n",
    "\n",
    "\n",
    "    def grounded_segmentation(\n",
    "        self,\n",
    "        image: Union[Image.Image],\n",
    "        labels: List[str],\n",
    "        threshold: float = 0.3,\n",
    "        polygon_refinement: bool = False,\n",
    "    ) -> Tuple[np.ndarray, List[DetectionResult]]:\n",
    "\n",
    "        detections = self.detect(image, labels, threshold)\n",
    "        detections = self.segment(image, detections, polygon_refinement)\n",
    "\n",
    "        return np.array(image), detections\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "segmentator = SegmentationEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23316cb017ed46ffa5e8404d60d0d06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d911628c3c4e8eb53bf02c2f8071a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8e79e92c8b4ed298f52af77837fabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85232210ccfe415c9546ab83b41d2555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfb68f124754dcaac3989009e3cb209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0f5bb38a2a48ed87f1c1a0491a9fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ee3cc90bc64ab0a2d9e4603a563383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5affd7580a4c468f6bfaa419858868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb049cd9d91495caa6b25716de4671b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4567b1ad314810b15c319109fa8f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a486f5a89624540b492ea6e0b4c32a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e81438092ef4d9b8c56bcbcd16b2e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a34b721da64952b058928db75356c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8426d35ce00a491db2a00756f3c365ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d968f83d744be988f5123af05c3dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/venv/main/lib/python3.10/site-packages/transformers/image_processing_utils.py:42: UserWarning: The following named arguments are not valid for `SamImageProcessor.preprocess` and were ignored: 'point_pad_value'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "  1%|          | 1/102 [00:01<01:42,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 271.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat001.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/102 [00:01<00:36,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 273.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat002.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.72 GiB is allocated by PyTorch, and 423.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat003.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5/102 [00:01<00:23,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 24.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 296.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat004.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 24.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.72 GiB is allocated by PyTorch, and 452.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat005.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/102 [00:02<00:20,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 273.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat006.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 332.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat007.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/102 [00:02<00:16,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 332.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat009.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 332.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat010.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      " 11%|█         | 11/102 [00:02<00:14,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 332.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat011.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 52.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 332.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat012.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/102 [00:02<00:13,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 301.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat013.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat014.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 15/102 [00:03<00:12,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat015.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat016.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/102 [00:03<00:11,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat017.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat018.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 19/102 [00:03<00:11,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat019.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat020.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/102 [00:04<00:12,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 22.81 MiB is free. Process 467730 has 23.61 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 360.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat021.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 50.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 273.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat022.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/102 [00:04<00:12,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 272.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat023.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat024.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 25/102 [00:04<00:11,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat025.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat026.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 26/102 [00:05<00:15,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat027.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 27/102 [00:05<00:19,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat028.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 28/102 [00:05<00:19,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat029.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 29/102 [00:06<00:21,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat030.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 30/102 [00:06<00:20,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat031.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 31/102 [00:06<00:21,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat032.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 32/102 [00:07<00:22,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat033.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 33/102 [00:07<00:23,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat034.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 34/102 [00:07<00:23,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat035.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 35/102 [00:08<00:23,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat036.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 36/102 [00:08<00:24,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat037.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 37/102 [00:09<00:27,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat038.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 39/102 [00:09<00:23,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat039.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat040.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 41/102 [00:10<00:17,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat041.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat042.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 42/102 [00:10<00:20,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat043.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 43/102 [00:11<00:22,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat044.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 44/102 [00:11<00:22,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat045.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 45/102 [00:11<00:19,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat046.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 46/102 [00:12<00:22,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat047.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 47/102 [00:12<00:21,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat048.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 48/102 [00:13<00:20,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat049.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 50/102 [00:13<00:14,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat050.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat051.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 51/102 [00:13<00:17,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat052.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 52/102 [00:14<00:25,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat053.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 53/102 [00:15<00:25,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat054.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 54/102 [00:15<00:23,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat055.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 55/102 [00:16<00:22,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat056.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 56/102 [00:16<00:19,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat057.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 57/102 [00:16<00:18,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat058.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 58/102 [00:17<00:18,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat059.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 59/102 [00:17<00:19,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat060.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 60/102 [00:18<00:19,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat061.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 61/102 [00:18<00:19,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat062.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 62/102 [00:19<00:16,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat063.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 63/102 [00:19<00:16,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat064.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 64/102 [00:20<00:15,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat065.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 65/102 [00:20<00:14,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat066.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 66/102 [00:20<00:15,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat067.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 67/102 [00:21<00:12,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat068.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 68/102 [00:21<00:11,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat069.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 69/102 [00:21<00:09,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat070.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 70/102 [00:22<00:11,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat071.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 71/102 [00:22<00:12,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat072.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 72/102 [00:22<00:11,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat073.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 73/102 [00:23<00:09,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat074.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 74/102 [00:23<00:10,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat075.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 75/102 [00:24<00:10,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat076.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 76/102 [00:24<00:10,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat077.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 77/102 [00:24<00:10,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat078.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 78/102 [00:25<00:10,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat079.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 79/102 [00:25<00:09,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat080.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 80/102 [00:26<00:08,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat081.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 81/102 [00:27<00:12,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat082.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 82/102 [00:27<00:11,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat083.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 83/102 [00:28<00:10,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 46.81 MiB is free. Process 467730 has 23.59 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 337.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat084.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 84/102 [00:28<00:08,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 269.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat085.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 85/102 [00:28<00:07,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat086.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 86/102 [00:29<00:07,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat087.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 87/102 [00:29<00:06,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat088.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 88/102 [00:30<00:06,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat089.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 89/102 [00:30<00:05,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat090.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 90/102 [00:31<00:05,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat091.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 91/102 [00:31<00:04,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat092.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 93/102 [00:32<00:03,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat093.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat094.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 94/102 [00:32<00:02,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat095.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 95/102 [00:33<00:03,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat096.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 96/102 [00:33<00:02,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat097.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 98/102 [00:33<00:01,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat098.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat099.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 100/102 [00:34<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat100.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat101.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:34<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat102.jpg\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 54.81 MiB is free. Process 467730 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 329.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "./processed_data/cat/cat103.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "labels = [\"cat\", \"dog\", \"fox\"]\n",
    "idx = 0\n",
    "\n",
    "\n",
    "image_list = sorted([\n",
    "    img for img in glob(f\"./processed_data/{labels[idx]}/*.jpg\") \n",
    "    if not img.endswith('_mask.jpg')\n",
    "])\n",
    "\n",
    "json_labeled = {}\n",
    "\n",
    "pipe = pipeline(\"image-text-to-text\", model=model_id, device_map=\"auto\")\n",
    "prompt = \"USER: <image>\\nDescribe what you see on this picture?\\nASSISTANT:\"\n",
    "\n",
    "for path in tqdm(image_list):\n",
    "    try:\n",
    "        image, pipe_res = segmentator.grounded_segmentation(Image.open(path).convert(\"RGB\"), labels=[labels[idx]])\n",
    "        print(pipe_res[0].box)\n",
    "        \n",
    "        outputs = pipe(Image.fromarray(image), text=prompt, max_new_tokens=200)\n",
    "        json_labeled[os.path.basename(path)[:-4]] = outputs[0][\"generated_text\"].split(\"ASSISTANT:\")[-1]\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
