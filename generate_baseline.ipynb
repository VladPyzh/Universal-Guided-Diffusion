{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"cat\", \"dog\", \"fox\"]\n",
    "idx = 2\n",
    "\n",
    "\n",
    "with open(f\"/job/processed_data/{labels[idx]}/annotations.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "for key, value in data.items():\n",
    "    image = pipe(value).images[0]  \n",
    "    image.save(f\"/job/generated_baseline/{labels[idx]}/{key}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict, Optional, Union, Tuple\n",
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    xmin: int\n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "\n",
    "    @property\n",
    "    def xyxy(self) -> List[float]:\n",
    "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    score: float\n",
    "    label: str\n",
    "    box: BoundingBox\n",
    "    mask: Optional[np.array] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
    "        return cls(score=detection_dict['score'],\n",
    "                   label=detection_dict['label'],\n",
    "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
    "                                   ymin=detection_dict['box']['ymin'],\n",
    "                                   xmax=detection_dict['box']['xmax'],\n",
    "                                   ymax=detection_dict['box']['ymax']))\n",
    "\n",
    "def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Extract the vertices of the contour\n",
    "    polygon = largest_contour.reshape(-1, 2).tolist()\n",
    "\n",
    "    return polygon\n",
    "\n",
    "def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a polygon to a segmentation mask.\n",
    "\n",
    "    Args:\n",
    "    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n",
    "    - image_shape (tuple): Shape of the image (height, width) for the mask.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Segmentation mask with the polygon filled.\n",
    "    \"\"\"\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Convert polygon to an array of points\n",
    "    pts = np.array(polygon, dtype=np.int32)\n",
    "\n",
    "    # Fill the polygon with white color (255)\n",
    "    cv2.fillPoly(mask, [pts], color=(255,))\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_boxes(results: DetectionResult) -> List[List[List[float]]]:\n",
    "    boxes = []\n",
    "    for result in results:\n",
    "        xyxy = result.box.xyxy\n",
    "        boxes.append(xyxy)\n",
    "\n",
    "    return [boxes]\n",
    "\n",
    "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
    "    masks = masks.cpu().float()\n",
    "    masks = masks.permute(0, 2, 3, 1)\n",
    "    masks = masks.mean(axis=-1)\n",
    "    masks = (masks > 0).int()\n",
    "    masks = masks.numpy().astype(np.uint8)\n",
    "    masks = list(masks)\n",
    "\n",
    "    if polygon_refinement:\n",
    "        for idx, mask in enumerate(masks):\n",
    "            shape = mask.shape\n",
    "            polygon = mask_to_polygon(mask)\n",
    "            mask = polygon_to_mask(polygon, shape)\n",
    "            masks[idx] = mask\n",
    "\n",
    "    return masks\n",
    "\n",
    "class SegmentationEngine:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.detector_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "        self.object_detector = pipeline(model=self.detector_id, \n",
    "                                        task=\"zero-shot-object-detection\", \n",
    "                                        device=self.device)\n",
    "\n",
    "        self.segmenter_id = \"facebook/sam-vit-huge\"\n",
    "\n",
    "        self.segmentator = AutoModelForMaskGeneration.from_pretrained(self.segmenter_id).to(self.device)\n",
    "        self.seg_processor = AutoProcessor.from_pretrained(self.segmenter_id)\n",
    "\n",
    "\n",
    "    def detect(\n",
    "            self,\n",
    "            image: Image.Image,\n",
    "            labels: List[str],\n",
    "            threshold: float = 0.5,\n",
    "    ):\n",
    "        \n",
    "        labels = [label if label.endswith(\".\") else label+\".\" for label in labels]\n",
    "\n",
    "        results = self.object_detector(image,  candidate_labels=labels, threshold=threshold)\n",
    "        results = [DetectionResult.from_dict(result) for result in results]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def segment(\n",
    "            self,\n",
    "            image: Image.Image,\n",
    "            detection_results: List[Dict[str, Any]],\n",
    "            polygon_refinement: bool = False,\n",
    "            segmenter_id: Optional[str] = None,\n",
    "    ):\n",
    "        boxes = get_boxes(detection_results)\n",
    "        inputs = self.seg_processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        outputs = self.segmentator(**inputs)\n",
    "        masks = self.seg_processor.post_process_masks(\n",
    "            masks=outputs.pred_masks,\n",
    "            original_sizes=inputs.original_sizes,\n",
    "            reshaped_input_sizes=inputs.reshaped_input_sizes\n",
    "        )[0]\n",
    "\n",
    "        masks = refine_masks(masks, polygon_refinement)\n",
    "\n",
    "        for detection_result, mask in zip(detection_results, masks):\n",
    "            detection_result.mask = mask\n",
    "\n",
    "        return detection_results\n",
    "\n",
    "\n",
    "    def grounded_segmentation(\n",
    "        self,\n",
    "        image: Union[Image.Image],\n",
    "        labels: List[str],\n",
    "        threshold: float = 0.3,\n",
    "        polygon_refinement: bool = False,\n",
    "    ) -> Tuple[np.ndarray, List[DetectionResult]]:\n",
    "\n",
    "        detections = self.detect(image, labels, threshold)\n",
    "        detections = self.segment(image, detections, polygon_refinement)\n",
    "\n",
    "        return np.array(image), detections\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "segmentator = SegmentationEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 71/102 [00:43<00:15,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "/job/generated_baseline/fox/fox071.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [01:02<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"cat\", \"dog\", \"fox\"]\n",
    "idx = 2\n",
    "\n",
    "\n",
    "image_list = sorted(glob(f\"/job/generated_baseline/{labels[idx]}/*.jpg\"))\n",
    "\n",
    "for path in tqdm(image_list):\n",
    "    try:\n",
    "        image, pipe_res = segmentator.grounded_segmentation(Image.open(path), labels=[labels[idx]])\n",
    "        plt.imsave(f\"/job/generated_baseline/{labels[idx]}/{os.path.basename(path)[:-4]}_mask.jpg\", pipe_res[0].mask)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
